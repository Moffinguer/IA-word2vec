{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58ae13b",
   "metadata": {},
   "source": [
    "# Generación de un corpus limitado a un umbral determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174adf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(input_file, output_file, umbral = 5):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "    ## Con gensim no es necesario usar la función build_vocab, el constructor lo construye en base al array de palabras\n",
    "    corpus_frequency = {}\n",
    "    for line in lines[1:]:\n",
    "        for word in line.split(\"\\t\")[0].split(\" \"):\n",
    "            if word in corpus_frequency:\n",
    "                corpus_frequency[word] += 1\n",
    "                continue\n",
    "            corpus_frequency[word] = 1\n",
    "\n",
    "    # Eliminar palabras que aparezcan menos veces de la esperada\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"sinopsis\\tgenero\\n\")\n",
    "        for line in lines[1:]:\n",
    "            synopsis, genre = line.split(\"\\t\")\n",
    "            file.write(f\"{synopsis}\\t{genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18051d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/stemming_data.txt'\n",
    "outpu_file = '../data/corpus_data.txt'\n",
    "corpus(input_file, outpu_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce2a98",
   "metadata": {},
   "source": [
    "Preparamos una sección para tokenizar las palabras de los ejemplos de prueba que se vayan probando, y así aumentar la precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e578d688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Obtener la lista de stopwords en español\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "special_chars = [ \"''\", \"...\", \"``\", \"<<\", \">>\", '\"\"', \"”\", \"“\"]\n",
    "category_alias = {}\n",
    "\n",
    "def word_accepted(word):\n",
    "    return word.lower() not in stop_words and word[0] not in string.punctuation and word[-1] not in string.punctuation and word not in special_chars\n",
    "\n",
    "def vectorization(model_type, model = None, corpus = None):\n",
    "    \n",
    "    ## Vectorizamos las palabras\n",
    "    if model_type in ['CBOW','SG']:\n",
    "        keywords = model.wv.index_to_key\n",
    "        vector = []\n",
    "        tmp = []\n",
    "        print\n",
    "        for text in corpus:\n",
    "            for word in text:\n",
    "                if word in keywords:\n",
    "                    tmp += [model.wv[word]]\n",
    "            vector += [tmp]\n",
    "            tmp = []\n",
    "                \n",
    "        return vector\n",
    "    elif model_type == 'BAYES':\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "    return CountVectorizer()\n",
    "\n",
    "\n",
    "def categorize(category, inverse = 0):\n",
    "    if not inverse:\n",
    "        if category in category_alias:\n",
    "            return category_alias[category]\n",
    "        if category_alias:\n",
    "            category_alias[category] = max(category_alias.values()) + 1\n",
    "        else:\n",
    "            category_alias[category] = 0\n",
    "        return category_alias[category]\n",
    "    else:\n",
    "        for translation, alias in category_alias.items():\n",
    "            if alias == category:\n",
    "                return translation\n",
    "        return \"None\"\n",
    "\n",
    "def normalize(text):\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    \n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    \n",
    "    tokenize = []\n",
    "    for s in text:\n",
    "        tmp = [token.lower() for token in word_tokenize(s) if word_accepted(token) and not re.search(\"^\\s*\\d+\\s*$\", token) ]\n",
    "        tokenize.append([stemmer.stem(token) for token in nltk.word_tokenize(' '.join(tmp))])\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30534d7b",
   "metadata": {},
   "source": [
    "# Función que hace el entrenamiento de las palabras del corpus\n",
    "### En base a los 3 tipos de modelos con los que vamos a trabajar creamos una función genérica que los abarque\n",
    "\n",
    "Aquellos modelos de la librería de Gensim, Bolsa de palabras y Skip Gram, son entrenados con el corpus limitado.\n",
    "Al necesitar vectores de palabras, que indique características de las palabras en base a valores numéricos, ya que no puede analizar cadenas como tal, requerimos de un proceso de vectorización, de esta forma y calculando la media de cada una de las propiedades de cada palabra, obtendremos un vector unidimensional por cada una las sinopsis de las peliculas.\n",
    "\n",
    "En el modelo de Bayes, repartimos el corpus en 2 secciones de manera que usando las facilidades de la librería de SKLearn, vectorizamos las palabras del corpus de entremiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f2460b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a model to train\n",
    "def training(input_file, model_type='CBOW'):  \n",
    "    if model_type in ['CBOW','SG']:\n",
    "        \n",
    "        # Obtener listado de palabras por cada pelicula de los ejemplos\n",
    "        corpus = []\n",
    "        topic = []\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        for line in lines[1:]:\n",
    "            tmp = line.split(\"\\t\")\n",
    "            corpus += [tmp[0].split(\" \")]\n",
    "            topic += [tmp[1].strip()]\n",
    "        \n",
    "        from gensim.models import Word2Vec\n",
    "        model = Word2Vec(sentences=corpus, sg = ( 0 if model_type == 'CBOW' else 1 ), epochs=500, seed=673721 )\n",
    "        vector = vectorization(model_type, model, corpus)\n",
    "        \n",
    "        # Calculamos una media para normalizar y quedarnos con vectores de 5 elementos por cada ejemplo\n",
    "        import numpy as np\n",
    "        weight = [np.mean(np.array(weights), axis=0) for weights in vector]\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "        clf.fit(weight, [categorize(theme) for theme in topic])\n",
    "        return (model, weight, clf)    \n",
    "    \n",
    "    elif model_type == \"BAYES\":\n",
    "        import pandas as pd \n",
    "        df = pd.read_csv(input_file, delimiter='\\t')\n",
    "       \n",
    "        ## Convertimos cada genero a un valor numérico\n",
    "        df['genero'] = df['genero'].apply( lambda c : categorize(c) )\n",
    "\n",
    "        ## Entrenamiento, creando vectores por las palabras\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        films_train, films_test, response_train, response_test = train_test_split( df.sinopsis, df.genero, test_size = 0.25)\n",
    "         \n",
    "        vector = vectorization(model_type)\n",
    "        films_train_count = vector.fit_transform(films_train)\n",
    "        \n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        model = MultinomialNB()\n",
    "        model.fit(films_train_count, response_train)\n",
    "        return (model,vector)\n",
    "    \n",
    "    else:\n",
    "        print(f\"ERROR unknown model {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7597d50",
   "metadata": {},
   "source": [
    "# Función que predice el tipo de película\n",
    "### En base a los 3 tipos de modelos con los que vamos a trabajar creamos una función genérica que los abarque\n",
    "\n",
    "La idea es similar en los 3 modelos, al tomar las sinopsis y tokenizarlas, buscamos encontrar la mayor similitud sobre cada vector y una categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90d554f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(synopsis, model, vector, clf ,type_model = 'CBOW', expected_output = []):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    import pandas as pd \n",
    "    df = pd.read_csv(input_file, delimiter='\\t')\n",
    "\n",
    "    ## Convertimos cada genero a un valor numérico\n",
    "    df['genero'] = df['genero'].apply( lambda c : categorize(c) )\n",
    "\n",
    "    \n",
    "    if type(synopsis) is str:\n",
    "        synopsis = [synopsis]\n",
    "   \n",
    "    if type_model in ['CBOW','SG']:\n",
    "        \n",
    "        films_train, films_test, response_train, response_test = train_test_split( df.sinopsis, df.genero, test_size = 0.25, random_state=42)\n",
    "        \n",
    "        synopsis = normalize(films_test)\n",
    "        temp_vector = vectorization(type_model, model, synopsis)\n",
    "       \n",
    "        import numpy as np\n",
    "        weight = [np.mean(np.array(weights), axis=0) for weights in temp_vector]\n",
    "\n",
    "        predictions = clf.predict(weight)\n",
    "        \n",
    "    elif type_model == \"BAYES\":\n",
    "        \n",
    "        films_train, films_test, response_train, response_test = train_test_split( df.sinopsis, df.genero, test_size = 0.25)\n",
    "        \n",
    "        synopsis = vector.transform(films_test)\n",
    "        predictions = model.predict(synopsis)\n",
    "        \n",
    "    else:\n",
    "        print(f\"ERROR unknown model {type_model}\")\n",
    "        \n",
    "    accuracy = accuracy_score(response_test, predictions)\n",
    "    matrix = confusion_matrix(response_test, predictions)\n",
    "    report = classification_report(response_test, predictions, zero_division=1)\n",
    "\n",
    "    print('ACCURACY: ', accuracy)\n",
    "    print('MATRIX CONFUSION: ')\n",
    "    print(matrix)\n",
    "    print('---------------------------------------------------------------')\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1fca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca30ef5",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c6f81aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usamos Bayes Multinomial\n",
      "ACCURACY:  0.8141592920353983\n",
      "MATRIX CONFUSION: \n",
      "[[13  0  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  5  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 42  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  4  6  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  2  0  6  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  4  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  3  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]\n",
      "---------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.79        16\n",
      "           1       0.83      0.56      0.67         9\n",
      "           2       0.74      0.98      0.84        43\n",
      "           3       0.86      0.55      0.67        11\n",
      "           4       1.00      0.67      0.80         9\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       1.00      1.00      1.00         2\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      0.75      0.86         4\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      0.50      0.67         2\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          16       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81       113\n",
      "   macro avg       0.95      0.77      0.81       113\n",
      "weighted avg       0.84      0.81      0.80       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../data/data.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "examples = lines[1:int(len(lines) / 2):15]\n",
    "expected_output = [ theme.split(\"\\t\")[1] for theme in examples]\n",
    "examples = [ synopsis.split(\"\\t\")[0] for synopsis in examples]\n",
    "input_file = '../data/corpus_data.txt'\n",
    "\n",
    "\n",
    "print(\"Usamos Bayes Multinomial\")\n",
    "(model,vector) = training(input_file, \"BAYES\")\n",
    "clf = None\n",
    "\n",
    "predict(examples, model, vector, clf, 'BAYES', expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "97db7073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.7964601769911505\n",
      "MATRIX CONFUSION: \n",
      "[[ 6  1  6  1  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 11  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  1 29  1  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  2  0  8  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0 11  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  2]]\n",
      "---------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.40      0.52        15\n",
      "           1       0.73      0.85      0.79        13\n",
      "           2       0.72      0.85      0.78        34\n",
      "           3       0.73      0.80      0.76        10\n",
      "           4       0.92      0.73      0.81        15\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      1.00      1.00         4\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       0.80      1.00      0.89         4\n",
      "          11       0.50      1.00      0.67         1\n",
      "          13       1.00      0.67      0.80         3\n",
      "          16       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80       113\n",
      "   macro avg       0.87      0.88      0.86       113\n",
      "weighted avg       0.81      0.80      0.79       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Bolsa de palabras\n",
    "(model, vector, clf) = training(input_file, 'CBOW')\n",
    "predict(examples, model, vector, clf, 'CBOW', expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "38e9d319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usamos SG:\n",
      "ACCURACY:  0.8407079646017699\n",
      "MATRIX CONFUSION: \n",
      "[[ 9  0  5  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  1 29  1  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 1  1  1  7  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 15  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  2  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  4  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  2]]\n",
      "---------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67        15\n",
      "           1       0.86      0.92      0.89        13\n",
      "           2       0.81      0.85      0.83        34\n",
      "           3       0.70      0.70      0.70        10\n",
      "           4       0.88      1.00      0.94        15\n",
      "           5       1.00      0.50      0.67         4\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      1.00      1.00         4\n",
      "           9       1.00      0.80      0.89         5\n",
      "          10       0.80      1.00      0.89         4\n",
      "          11       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         3\n",
      "          16       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.84       113\n",
      "   macro avg       0.91      0.88      0.89       113\n",
      "weighted avg       0.84      0.84      0.84       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Usamos SG:\")\n",
    "## SG\n",
    "(model, vector, clf) = training(input_file, 'SG')\n",
    "predict(examples, model, vector, clf, 'SG', expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e633f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33c4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30205fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82434949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077887ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
