{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cce2a98",
   "metadata": {},
   "source": [
    "En esta sección prepararemos los modelos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e9cd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_alias = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e578d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(model_type, model = None, corpus = None):\n",
    "    \n",
    "    ## Vectorizamos las palabras\n",
    "    if model_type in ['CBOW','SG']:\n",
    "        keywords = model.wv.index_to_key\n",
    "        vector = []\n",
    "        tmp = []\n",
    "        for text in corpus:\n",
    "            for word in text:\n",
    "                if word in keywords:\n",
    "                    tmp += [model.wv[word]]\n",
    "            vector += [tmp]\n",
    "            tmp = []\n",
    "        return vector\n",
    "    elif model_type == 'BAYES':\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "    return CountVectorizer()\n",
    "\n",
    "\n",
    "def categorize(category, inverse = 0):\n",
    "    if not inverse:\n",
    "        if category in category_alias:\n",
    "            return category_alias[category]\n",
    "        if category_alias:\n",
    "            category_alias[category] = max(category_alias.values()) + 1\n",
    "        else:\n",
    "            category_alias[category] = 0\n",
    "        return category_alias[category]\n",
    "    else:\n",
    "        for translation, alias in category_alias.items():\n",
    "            if alias == category:\n",
    "                return translation\n",
    "        return \"None\"\n",
    "\n",
    "def training(input_file, model_type='CBOW'):  \n",
    "    ## Load a model to train\n",
    "    \n",
    "    if model_type in ['CBOW','SG']:\n",
    "        \n",
    "        # Obtener listado de palabras por cada pelicula de los ejemplos\n",
    "        corpus = []\n",
    "        topic = []\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        for line in lines[:1]:\n",
    "            tmp = line.split(\"\\t\")\n",
    "            corpus += [tmp[0].split(\" \")]\n",
    "            topic += [tmp[1].strip()]\n",
    "        \n",
    "        from gensim.models import Word2Vec\n",
    "        model = Word2Vec(sentences=corpus, sg = ( 0 if model_type == 'CBOW' else 1 ) )\n",
    "        vector = vectorization(model_type, model, corpus)\n",
    "        \n",
    "        \n",
    "        # Calculamos una media para normalizar y quedarnos con vectores de 5 elementos por cada ejemplo\n",
    "        import numpy as np\n",
    "        weight = [np.mean(np.array(weights), axis=0) for weights in vector]\n",
    "        \n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "        clf.fit(weight, categorize(topic))\n",
    "        \n",
    "        return (model, weight, clf)    \n",
    "    elif model_type == \"BAYES\":\n",
    "        import pandas as pd \n",
    "        df = pd.read_csv(input_file, delimiter='\\t')\n",
    "       \n",
    "        ## Convertimos cada genero a un valor numérico\n",
    "        df['genero'] = df['genero'].apply( lambda c : categorize(c) )\n",
    "\n",
    "        ## Entrenamiento, creando vectores por las palabras\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        films_train, films_test, response_train, response_test = train_test_split( df.sinopsis, df.genero, test_size = .25)\n",
    "         \n",
    "        vector = vectorization(model_type)\n",
    "        films_train_count = vector.fit_transform(films_train)\n",
    "        \n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        model = MultinomialNB()\n",
    "        model.fit(films_train_count, response_train)\n",
    "        \n",
    "        return (model,vector)\n",
    "    else:\n",
    "        print(f\"ERROR unknown model {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2460b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(synopsis, model, vector, clf ,type_model = 'CBOW'):\n",
    "    if type(synopsis) is str:\n",
    "        synopsis = [synopsis]\n",
    "    \n",
    "    if type_model in ['CBOW','SG']:\n",
    "        \n",
    "        # Aquí deberíamos de aplicar la función de tokenización\n",
    "        synopsis = [ text.split(\" \") for text in synopsis ]\n",
    "        temp_vector = vectorization(type_model, model)\n",
    "        \n",
    "        import numpy as np\n",
    "        weights = [np.mean(np.array(v), axis=0) for v in temp_vector]\n",
    "        predictions = clf.predict(weights)\n",
    "        \n",
    "    elif type_model == \"BAYES\":\n",
    "        if type(synopsis) is str:\n",
    "            synopsis = [synopsis]\n",
    "        synopsis = vector.transform(synopsis)\n",
    "        predictions = model.predict(synopsis)\n",
    "    else:\n",
    "        print(f\"ERROR unknown model {type_model}\")\n",
    "        \n",
    "    for category in predictions:\n",
    "        print(f\"Film has a category of {categorize(category, 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca30ef5",
   "metadata": {},
   "source": [
    "Testeo con predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00cca831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Film has a category of Drama\n"
     ]
    }
   ],
   "source": [
    "input_file = '../data/stemming_data.txt'\n",
    "\n",
    "## Para los otros modelos, deben de devolver un parametro extra (model, vector, clf)\n",
    "(model,vector) = training(input_file, \"BAYES\")\n",
    "clf = None\n",
    "\n",
    "predict('Las Aes Sadai, una poderosa fortaleza de mujeres, parecen dominar la magia por su capacidad de contactar con el Poder Único que se obtiene de la Fuente Verdadera, que hace girar la vital Rueda del Tiempo. La Época de la locura ha llegado por la contaminación de una parte de la fuente dejando un mundo arruinado y desorganizado en su forma de vida', model, vector, clf, 'BAYES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be6f3727",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/stemming_data.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## Para los otros modelos, deben de devolver un parametro extra (model, vector, clf)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m (model,vector,clf) \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCBOW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m predict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLas Aes Sadai, una poderosa fortaleza de mujeres, parecen dominar la magia por su capacidad de contactar con el Poder Único que se obtiene de la Fuente Verdadera, que hace girar la vital Rueda del Tiempo. La Época de la locura ha llegado por la contaminación de una parte de la fuente dejando un mundo arruinado y desorganizado en su forma de vida\u001b[39m\u001b[38;5;124m'\u001b[39m, model, vector, clf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAYES\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 51\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(input_file, model_type)\u001b[0m\n\u001b[1;32m     48\u001b[0m     topic \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [tmp[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m---> 51\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCBOW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m vector \u001b[38;5;241m=\u001b[39m vectorization(model_type, model, corpus)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Calculamos una media para normalizar y quedarnos con vectores de 5 elementos por cada ejemplo\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gensim/models/word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_total_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gensim/models/word2vec.py:1045\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha \u001b[38;5;241m=\u001b[39m end_alpha \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_training_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1050\u001b[0m     msg\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     ),\n\u001b[1;32m   1055\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gensim/models/word2vec.py:1554\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffective \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m higher than previous training cycles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mkey_to_index:  \u001b[38;5;66;03m# should be set by `build_vocab`\u001b[39;00m\n\u001b[0;32m-> 1554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must first build vocabulary before training the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors):\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must initialize vectors before training the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "input_file = '../data/stemming_data.txt'\n",
    "\n",
    "## Para los otros modelos, deben de devolver un parametro extra (model, vector, clf)\n",
    "(model,vector,clf) = training(input_file, \"CBOW\")\n",
    "\n",
    "predict('Las Aes Sadai, una poderosa fortaleza de mujeres, parecen dominar la magia por su capacidad de contactar con el Poder Único que se obtiene de la Fuente Verdadera, que hace girar la vital Rueda del Tiempo. La Época de la locura ha llegado por la contaminación de una parte de la fuente dejando un mundo arruinado y desorganizado en su forma de vida', model, vector, clf, 'BAYES')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
