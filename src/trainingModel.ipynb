{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58ae13b",
   "metadata": {},
   "source": [
    "# Generación de un corpus limitado a un umbral determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "174adf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(input_file, output_file, umbral = 5):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "    ## Con gensim no es necesario usar la función build_vocab, el constructor lo construye en base al array de palabras\n",
    "    corpus_frequency = {}\n",
    "    for line in lines[1:]:\n",
    "        for word in line.split(\"\\t\")[0].split(\" \"):\n",
    "            if word in corpus_frequency:\n",
    "                corpus_frequency[word] += 1\n",
    "                continue\n",
    "            corpus_frequency[word] = 1\n",
    "\n",
    "    # Eliminar palabras que aparezcan menos veces de la esperada\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"sinopsis\\tgenero\\n\")\n",
    "        for line in lines[1:]:\n",
    "            synopsis, genre = line.split(\"\\t\")\n",
    "            file.write(f\"{synopsis}\\t{genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d18051d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/stemming_data.txt'\n",
    "outpu_file = '../data/corpus_data.txt'\n",
    "corpus(input_file, outpu_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce2a98",
   "metadata": {},
   "source": [
    "Preparamos una sección para tokenizar las palabras de los ejemplos de prueba que se vayan probando, y así aumentar la precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e578d688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/moffinguer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/moffinguer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Obtener la lista de stopwords en español\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "special_chars = [ \"''\", \"...\", \"``\", \"<<\", \">>\", '\"\"', \"”\", \"“\"]\n",
    "category_alias = {}\n",
    "\n",
    "def word_accepted(word):\n",
    "    return word.lower() not in stop_words and word[0] not in string.punctuation and word[-1] not in string.punctuation and word not in special_chars\n",
    "\n",
    "def vectorization(model_type, model = None, corpus = None):\n",
    "    \n",
    "    ## Vectorizamos las palabras\n",
    "    if model_type in ['CBOW','SG']:\n",
    "        keywords = model.wv.index_to_key\n",
    "        vector = []\n",
    "        tmp = []\n",
    "        print\n",
    "        for text in corpus:\n",
    "            for word in text:\n",
    "                if word in keywords:\n",
    "                    tmp += [model.wv[word]]\n",
    "            vector += [tmp]\n",
    "            tmp = []\n",
    "                \n",
    "        return vector\n",
    "    elif model_type == 'BAYES':\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "    return CountVectorizer()\n",
    "\n",
    "\n",
    "def categorize(category, inverse = 0):\n",
    "    if not inverse:\n",
    "        if category in category_alias:\n",
    "            return category_alias[category]\n",
    "        if category_alias:\n",
    "            category_alias[category] = max(category_alias.values()) + 1\n",
    "        else:\n",
    "            category_alias[category] = 0\n",
    "        return category_alias[category]\n",
    "    else:\n",
    "        for translation, alias in category_alias.items():\n",
    "            if alias == category:\n",
    "                return translation\n",
    "        return \"None\"\n",
    "\n",
    "def normalize(text):\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    \n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    \n",
    "    tokenize = []\n",
    "    for s in text:\n",
    "        tmp = [token.lower() for token in word_tokenize(s) if word_accepted(token) and not re.search(\"^\\s*\\d+\\s*$\", token) ]\n",
    "        tokenize.append([stemmer.stem(token) for token in nltk.word_tokenize(' '.join(tmp))])\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30534d7b",
   "metadata": {},
   "source": [
    "# Función que hace el entrenamiento de las palabras del corpus\n",
    "### En base a los 3 tipos de modelos con los que vamos a trabajar creamos una función genérica que los abarque\n",
    "\n",
    "Aquellos modelos de la librería de Gensim, Bolsa de palabras y Skip Gram, son entrenados con el corpus limitado.\n",
    "Al necesitar vectores de palabras, que indique características de las palabras en base a valores numéricos, ya que no puede analizar cadenas como tal, requerimos de un proceso de vectorización, de esta forma y calculando la media de cada una de las propiedades de cada palabra, obtendremos un vector unidimensional por cada una las sinopsis de las peliculas.\n",
    "\n",
    "En el modelo de Bayes, repartimos el corpus en 2 secciones de manera que usando las facilidades de la librería de SKLearn, vectorizamos las palabras del corpus de entremiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2460b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a model to train\n",
    "def training(input_file, model_type='CBOW'):  \n",
    "    if model_type in ['CBOW','SG']:\n",
    "        \n",
    "        # Obtener listado de palabras por cada pelicula de los ejemplos\n",
    "        corpus = []\n",
    "        topic = []\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        for line in lines[1:]:\n",
    "            tmp = line.split(\"\\t\")\n",
    "            corpus += [tmp[0].split(\" \")]\n",
    "            topic += [tmp[1].strip()]\n",
    "        \n",
    "        from gensim.models import Word2Vec\n",
    "        model = Word2Vec(sentences=corpus, sg = ( 0 if model_type == 'CBOW' else 1 ), epochs=200, seed=673721 )\n",
    "        vector = vectorization(model_type, model, corpus)\n",
    "        \n",
    "        # Calculamos una media para normalizar y quedarnos con vectores de 5 elementos por cada ejemplo\n",
    "        import numpy as np\n",
    "        weight = [np.mean(np.array(weights), axis=0) for weights in vector]\n",
    "        \n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "        clf.fit(weight, [categorize(theme) for theme in topic])\n",
    "        return (model, weight, clf)    \n",
    "    \n",
    "    elif model_type == \"BAYES\":\n",
    "        import pandas as pd \n",
    "        df = pd.read_csv(input_file, delimiter='\\t')\n",
    "       \n",
    "        ## Convertimos cada genero a un valor numérico\n",
    "        df['genero'] = df['genero'].apply( lambda c : categorize(c) )\n",
    "\n",
    "        ## Entrenamiento, creando vectores por las palabras\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        films_train, films_test, response_train, response_test = train_test_split( df.sinopsis, df.genero, test_size = .25)\n",
    "         \n",
    "        vector = vectorization(model_type)\n",
    "        films_train_count = vector.fit_transform(films_train)\n",
    "        \n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        model = MultinomialNB()\n",
    "        model.fit(films_train_count, response_train)\n",
    "        return (model,vector)\n",
    "    \n",
    "    else:\n",
    "        print(f\"ERROR unknown model {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7597d50",
   "metadata": {},
   "source": [
    "# Función que predice el tipo de película\n",
    "### En base a los 3 tipos de modelos con los que vamos a trabajar creamos una función genérica que los abarque\n",
    "\n",
    "La idea es similar en los 3 modelos, al tomar las sinopsis y tokenizarlas, buscamos encontrar la mayor similitud sobre cada vector y una categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d554f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(synopsis, model, vector, clf ,type_model = 'CBOW', expected_output = []):\n",
    "    if type(synopsis) is str:\n",
    "        synopsis = [synopsis]\n",
    "   \n",
    "    if type_model in ['CBOW','SG']:\n",
    "        \n",
    "        synopsis = normalize(synopsis)\n",
    "        temp_vector = vectorization(type_model, model, synopsis)\n",
    "       \n",
    "        import numpy as np\n",
    "        weight = [np.mean(np.array(weights), axis=0) for weights in temp_vector]\n",
    "\n",
    "        predictions = clf.predict(weight)\n",
    "        \n",
    "    elif type_model == \"BAYES\":\n",
    "        tmp = []\n",
    "        for i in synopsis:\n",
    "            tmp += [' '.join(tmp)]\n",
    "        synopsis = tmp\n",
    "        \n",
    "        synopsis = vector.transform(synopsis)\n",
    "        predictions = model.predict(synopsis)\n",
    "        \n",
    "    else:\n",
    "        print(f\"ERROR unknown model {type_model}\")\n",
    "        \n",
    "    accuracy = 0\n",
    "    for i in range(len(expected_output)):\n",
    "        predict = categorize(predictions[i], 1)\n",
    "        accuracy += ( predict in expected_output[i])\n",
    "        print(f\"Film has a category of {predict}. Expected a category of {expected_output[i]}\")\n",
    "        \n",
    "    print(f\"The model {type_model} has an accuracy of {100 * accuracy / len(expected_output)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca30ef5",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6f81aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usamos Bayes Multinomial\n",
      "Film has a category of Drama. Expected a category of Comedia\n",
      "\n",
      "Film has a category of Drama. Expected a category of Comedia\n",
      "\n",
      "Film has a category of Drama. Expected a category of Suspense\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Drama. Expected a category of Documental\n",
      "\n",
      "Film has a category of Drama. Expected a category of Aventura\n",
      "\n",
      "Film has a category of Drama. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Drama. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Drama. Expected a category of Acción\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Drama. Expected a category of Acción\n",
      "\n",
      "Film has a category of Drama. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Drama. Expected a category of Fantasía\n",
      "\n",
      "The model BAYES has an accuracy of 26.666666666666668\n",
      "\n",
      "Usamos bolsa de palabras ahora:\n",
      "Film has a category of Drama. Expected a category of Comedia\n",
      "\n",
      "Film has a category of Comedia. Expected a category of Comedia\n",
      "\n",
      "Film has a category of Suspense. Expected a category of Suspense\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Comedia. Expected a category of Documental\n",
      "\n",
      "Film has a category of Drama. Expected a category of Aventura\n",
      "\n",
      "Film has a category of Crimen. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Drama. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Crimen. Expected a category of Acción\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Acción. Expected a category of Acción\n",
      "\n",
      "Film has a category of Drama. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Fantasía. Expected a category of Fantasía\n",
      "\n",
      "The model CBOW has an accuracy of 60.0\n",
      "\n",
      "Usamos SG:\n",
      "Film has a category of Drama. Expected a category of Comedia\n",
      "\n",
      "Film has a category of Comedia. Expected a category of Comedia\n",
      "\n",
      "Film has a category of Suspense. Expected a category of Suspense\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Documental. Expected a category of Documental\n",
      "\n",
      "Film has a category of Drama. Expected a category of Aventura\n",
      "\n",
      "Film has a category of Crimen. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Drama. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Crimen. Expected a category of Acción\n",
      "\n",
      "Film has a category of Drama. Expected a category of Drama\n",
      "\n",
      "Film has a category of Acción. Expected a category of Acción\n",
      "\n",
      "Film has a category of Drama. Expected a category of Crimen\n",
      "\n",
      "Film has a category of Fantasía. Expected a category of Fantasía\n",
      "\n",
      "The model SG has an accuracy of 66.66666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../data/data.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "examples = lines[1:int(len(lines) / 2):15]\n",
    "expected_output = [ theme.split(\"\\t\")[1] for theme in examples]\n",
    "examples = [ synopsis.split(\"\\t\")[0] for synopsis in examples]\n",
    "\n",
    "\n",
    "print(\"Usamos Bayes Multinomial\")\n",
    "input_file = '../data/corpus_data.txt'\n",
    "(model,vector) = training(input_file, \"BAYES\")\n",
    "clf = None\n",
    "\n",
    "predict(examples, model, vector, clf, 'BAYES', expected_output)\n",
    "print(\"Usamos bolsa de palabras ahora:\")\n",
    "\n",
    "\n",
    "## Bolsa de palabras\n",
    "(model, vector, clf) = training(input_file, 'CBOW')\n",
    "predict(examples, model, vector, clf, 'CBOW', expected_output)\n",
    "\n",
    "\n",
    "print(\"Usamos SG:\")\n",
    "## SG\n",
    "(model, vector, clf) = training(input_file, 'SG')\n",
    "predict(examples, model, vector, clf, 'SG', expected_output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
