{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58ae13b",
   "metadata": {},
   "source": [
    "Generación de un corpus limitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174adf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(input_file, output_file, umbral = 5):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "    ## Con gensim no es necesario usar la función build_vocab, el constructor lo construye en base al array de palabras\n",
    "    corpus_frequency = {}\n",
    "    for line in lines[1:]:\n",
    "        for word in line.split(\"\\t\")[0].split(\" \"):\n",
    "            if word in corpus_frequency:\n",
    "                corpus_frequency[word] += 1\n",
    "                continue\n",
    "            corpus_frequency[word] = 1\n",
    "\n",
    "    # Eliminar palabras que aparezcan menos veces de la esperada\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"sinopsis\\tgenero\\n\")\n",
    "        for line in lines[1:]:\n",
    "            synopsis, genre = line.split(\"\\t\")\n",
    "            file.write(f\"{synopsis}\\t{genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18051d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/stemming_data.txt'\n",
    "outpu_file = '../data/corpus_data.txt'\n",
    "corpus(input_file, outpu_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce2a98",
   "metadata": {},
   "source": [
    "En esta sección prepararemos los modelos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e578d688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/moffinguer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/moffinguer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Obtener la lista de stopwords en español\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "special_chars = [ \"''\", \"...\", \"``\", \"<<\", \">>\", '\"\"', \"”\", \"“\"]\n",
    "category_alias = {}\n",
    "\n",
    "def word_accepted(word):\n",
    "    return word.lower() not in stop_words and word not in special_chars\n",
    "\n",
    "def vectorization(model_type, model = None, corpus = None):\n",
    "    \n",
    "    ## Vectorizamos las palabras\n",
    "    if model_type in ['CBOW','SG']:\n",
    "        keywords = model.wv.index_to_key\n",
    "        vector = []\n",
    "        tmp = []\n",
    "        print\n",
    "        for text in corpus:\n",
    "            for word in text:\n",
    "                if word in keywords:\n",
    "                    tmp += [model.wv[word]]\n",
    "            vector += [tmp]\n",
    "            tmp = []\n",
    "                \n",
    "        return vector\n",
    "    elif model_type == 'BAYES':\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "    return CountVectorizer()\n",
    "\n",
    "\n",
    "def categorize(category, inverse = 0):\n",
    "    if not inverse:\n",
    "        if category in category_alias:\n",
    "            return category_alias[category]\n",
    "        if category_alias:\n",
    "            category_alias[category] = max(category_alias.values()) + 1\n",
    "        else:\n",
    "            category_alias[category] = 0\n",
    "        return category_alias[category]\n",
    "    else:\n",
    "        for translation, alias in category_alias.items():\n",
    "            if alias == category:\n",
    "                return translation\n",
    "        return \"None\"\n",
    "\n",
    "def normalize(text):\n",
    "    import re\n",
    "    import nltk\n",
    "    import string\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    \n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    \n",
    "    tokenize = []\n",
    "    for s in text:\n",
    "        tmp = [token.lower() for token in word_tokenize(s) if word_accepted(token) and not re.search(\"^\\s*\\d+\\s*$\", token) ]\n",
    "        tokenize.append([stemmer.stem(token) for token in nltk.word_tokenize(' '.join(tmp))])\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2460b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a model to train\n",
    "def training(input_file, model_type='CBOW'):  \n",
    "    if model_type in ['CBOW','SG']:\n",
    "        \n",
    "        # Obtener listado de palabras por cada pelicula de los ejemplos\n",
    "        corpus = []\n",
    "        topic = []\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        for line in lines[1:]:\n",
    "            tmp = line.split(\"\\t\")\n",
    "            corpus += [tmp[0].split(\" \")]\n",
    "            topic += [tmp[1].strip()]\n",
    "        \n",
    "        from gensim.models import Word2Vec\n",
    "        model = Word2Vec(sentences=corpus, sg = ( 0 if model_type == 'CBOW' else 1 ) )\n",
    "        vector = vectorization(model_type, model, corpus)\n",
    "        \n",
    "        # Calculamos una media para normalizar y quedarnos con vectores de 5 elementos por cada ejemplo\n",
    "        import numpy as np\n",
    "        weight = [np.mean(np.array(weights), axis=0) for weights in vector]\n",
    "        \n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "        clf.fit(weight, [categorize(theme) for theme in topic])\n",
    "        return (model, weight, clf)    \n",
    "    \n",
    "    elif model_type == \"BAYES\":\n",
    "        import pandas as pd \n",
    "        df = pd.read_csv(input_file, delimiter='\\t')\n",
    "       \n",
    "        ## Convertimos cada genero a un valor numérico\n",
    "        df['genero'] = df['genero'].apply( lambda c : categorize(c) )\n",
    "\n",
    "        ## Entrenamiento, creando vectores por las palabras\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        films_train, films_test, response_train, response_test = train_test_split( df.sinopsis, df.genero, test_size = .25)\n",
    "         \n",
    "        vector = vectorization(model_type)\n",
    "        films_train_count = vector.fit_transform(films_train)\n",
    "        \n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        model = MultinomialNB()\n",
    "        model.fit(films_train_count, response_train)\n",
    "        return (model,vector)\n",
    "    \n",
    "    else:\n",
    "        print(f\"ERROR unknown model {model}\")\n",
    "\n",
    "def predict(synopsis, model, vector, clf ,type_model = 'CBOW'):\n",
    "    if type(synopsis) is str:\n",
    "        synopsis = [synopsis]\n",
    "   \n",
    "    if type_model in ['CBOW','SG']:\n",
    "        \n",
    "        synopsis = normalize(synopsis)\n",
    "        temp_vector = vectorization(type_model, model, synopsis)\n",
    "       \n",
    "        import numpy as np\n",
    "        weight = [np.mean(np.array(weights), axis=0) for weights in temp_vector]\n",
    "\n",
    "        predictions = clf.predict(weight)\n",
    "        \n",
    "    elif type_model == \"BAYES\":\n",
    "        tmp = []\n",
    "        for i in synopsis:\n",
    "            tmp += [' '.join(tmp)]\n",
    "        synopsis = tmp\n",
    "        \n",
    "        synopsis = vector.transform(synopsis)\n",
    "        predictions = model.predict(synopsis)\n",
    "        \n",
    "    else:\n",
    "        print(f\"ERROR unknown model {type_model}\")\n",
    "        \n",
    "    for category in predictions:\n",
    "        print(f\"Film has a category of {categorize(category, 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca30ef5",
   "metadata": {},
   "source": [
    "Testeo con predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00cca831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Film has a category of Drama\n"
     ]
    }
   ],
   "source": [
    "input_file = '../data/corpus_data.txt'\n",
    "\n",
    "## Para los otros modelos, deben de devolver un parametro extra (model, vector, clf)\n",
    "(model,vector) = training(input_file, \"BAYES\")\n",
    "clf = None\n",
    "\n",
    "predict('Las Aes Sadai, una poderosa fortaleza de mujeres, parecen dominar la magia por su capacidad de contactar con el Poder Único que se obtiene de la Fuente Verdadera, que hace girar la vital Rueda del Tiempo. La Época de la locura ha llegado por la contaminación de una parte de la fuente dejando un mundo arruinado y desorganizado en su forma de vida', model, vector, clf, 'BAYES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6f3727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Film has a category of Drama\n"
     ]
    }
   ],
   "source": [
    "input_file = '../data/corpus_data.txt'\n",
    "\n",
    "## Para los otros modelos, deben de devolver un parametro extra (model, vector, clf)\n",
    "(model,vector,clf) = training(input_file, \"CBOW\")\n",
    "\n",
    "predict('Las Aes Sadai, una poderosa fortaleza de mujeres, parecen dominar la magia por su capacidad de contactar con el Poder Único que se obtiene de la Fuente Verdadera, que hace girar la vital Rueda del Tiempo. La Época de la locura ha llegado por la contaminación de una parte de la fuente dejando un mundo arruinado y desorganizado en su forma de vida', model, vector, clf, 'CBOW')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
